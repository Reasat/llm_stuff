{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "aFiLM7wykc-x"
   },
   "outputs": [],
   "source": [
    "# !pip install -q -U bitsandbytes\n",
    "# !pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "# !pip install -q datasets loralib sentencepiece\n",
    "# !pip install -q gradio\n",
    "# !pip install -q ttsmms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KyuqHMbplDZy",
    "outputId": "78c8a15b-99af-4adb-ecd3-45ecf03fa7d5"
   },
   "outputs": [],
   "source": [
    "# # For TTS\n",
    "\n",
    "# !curl https://dl.fbaipublicfiles.com/mms/tts/ory.tar.gz --output ory.tar.gz #update lang\n",
    "# !mkdir -p data && tar -xzf ory.tar.gz -C data/ #update langcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.odiagenai.org/blog/odiagenai-released-the-first-llm-for-the-low-resource-odia-language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.43.0.dev0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/reasat/miniconda3/envs/phison/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:519: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly by `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation, and ensure your `input_ids` input does not have negative values.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdecf44d9b0f4a3eb3a679289b95ad6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f3ae54f2fe478a92bfcf5fbe925d61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/490 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40ec01c075347129a7c9715c55c44d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/71.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "\n",
    "# Load the base LLaMA model\n",
    "base_model_name = \"baffo32/decapoda-research-llama-7B-hf\"\n",
    "lora_weights_path = \"OdiaGenAI/odiagenAI-bengali-lora-model-v1\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "# Hey! It was indeed fixed for other models, but Llama is a bit specific, we call self.update_post_processor() which makes sure that the eos token is added. This should work without a bos token, but you can't have no unk_token, this is pretty much a requirement for all our tokenizers. Not sure I can fix this as raising an error would not be BC :/ would recommend doing this:\n",
    "\n",
    "tokenizer = transformers.LlamaTokenizer.from_pretrained('baffo32/decapoda-research-llama-7B-hf', unk_token=\"<unk>\") \n",
    "model = transformers.LlamaForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    # load_in_8bit=False,\n",
    "    quantization_config=transformers.BitsAndBytesConfig(load_in_8bit=True),\n",
    "    torch_dtype = torch.float16,\n",
    "    device_map = 'auto'\n",
    ")\n",
    "lora_model = PeftModel.from_pretrained(\n",
    "    model, \n",
    "    lora_weights_path,\n",
    "    force_download = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from peft import PeftModel, PeftConfig\n",
    "\n",
    "# # Path to the LoRA weights\n",
    "# lora_weights_path = \"OdiaGenAI/odiagenAI-bengali-lora-model-v1\"\n",
    "\n",
    "# # Load the LoRA configuration\n",
    "# config = PeftConfig.from_pretrained(lora_weights_path)\n",
    "\n",
    "# # Apply the LoRA weights to the base model\n",
    "# lora_model = PeftModel.from_pretrained(model, lora_weights_path, config=config)\n",
    "# lora_model = lora_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AOkQ4N9slzNI",
    "outputId": "90ce901d-0158-42a0-bcf3-dbc9dd5ac5cd"
   },
   "outputs": [],
   "source": [
    "# # import torch\n",
    "# from peft import PeftModel\n",
    "# # from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n",
    "\n",
    "\n",
    "# import torch\n",
    "# # from peft import PeftModel\n",
    "# import transformers\n",
    "# import gradio as gr\n",
    "\n",
    "# assert (\n",
    "#     \"LlamaTokenizer\" in transformers._import_structure[\"models.llama\"]\n",
    "# ), \"LLaMA is now in HuggingFace's main branch.\\nPlease reinstall it: pip uninstall transformers && pip install git+https://github.com/huggingface/transformers.git\"\n",
    "# from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "4236dcba501f4eb38ecc4edaa237da21",
      "48274de1ad484519ab406a162f1daf58",
      "67175ae3885a453fbccb9c455638547f",
      "39ff2366b890492ab3c62d405a2b3274",
      "6081eb420cb447e2baacc0e7804b7f53",
      "1e236e1dad0c411dbe4aa5e8c31694a1",
      "ec140945db204424afe0d801babffe60",
      "222ec815e1b04fafb94d7cf81a047b12",
      "5e0a638e8bc340c98e52642bf8c90fa2",
      "d67a75f9924647959277a99156825b93",
      "936c86ed69c3484681d9bd5a736d134b"
     ]
    },
    "id": "yeiHgt3UmpWY",
    "outputId": "ce23ceff-6f76-4d11-80f4-693cbe5c6c30"
   },
   "outputs": [],
   "source": [
    "# model_id = \"OdiaGenAI/odiagenAI_llama7b_base_v1\"\n",
    "# device = \"cuda:0\"\n",
    "\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "# model = LlamaForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")   # 4 bit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "4236dcba501f4eb38ecc4edaa237da21",
      "48274de1ad484519ab406a162f1daf58",
      "67175ae3885a453fbccb9c455638547f",
      "39ff2366b890492ab3c62d405a2b3274",
      "6081eb420cb447e2baacc0e7804b7f53",
      "1e236e1dad0c411dbe4aa5e8c31694a1",
      "ec140945db204424afe0d801babffe60",
      "222ec815e1b04fafb94d7cf81a047b12",
      "5e0a638e8bc340c98e52642bf8c90fa2",
      "d67a75f9924647959277a99156825b93",
      "936c86ed69c3484681d9bd5a736d134b"
     ]
    },
    "id": "yeiHgt3UmpWY",
    "outputId": "ce23ceff-6f76-4d11-80f4-693cbe5c6c30"
   },
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# if torch.__version__ >= \"2\":\n",
    "#     model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "4236dcba501f4eb38ecc4edaa237da21",
      "48274de1ad484519ab406a162f1daf58",
      "67175ae3885a453fbccb9c455638547f",
      "39ff2366b890492ab3c62d405a2b3274",
      "6081eb420cb447e2baacc0e7804b7f53",
      "1e236e1dad0c411dbe4aa5e8c31694a1",
      "ec140945db204424afe0d801babffe60",
      "222ec815e1b04fafb94d7cf81a047b12",
      "5e0a638e8bc340c98e52642bf8c90fa2",
      "d67a75f9924647959277a99156825b93",
      "936c86ed69c3484681d9bd5a736d134b"
     ]
    },
    "id": "yeiHgt3UmpWY",
    "outputId": "ce23ceff-6f76-4d11-80f4-693cbe5c6c30"
   },
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    instruction,\n",
    "    input=None,\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=40,\n",
    "    num_beams=4,\n",
    "    max_new_tokens=128,\n",
    "    **kwargs,\n",
    "):\n",
    "    prompt = generate_prompt(instruction, input)\n",
    "    print(prompt)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    print(inputs)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    print(input_ids)\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        num_beams=num_beams,\n",
    "        **kwargs,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "        # print(generation_output)\n",
    "    s = generation_output.sequences[0]\n",
    "    print(s)\n",
    "    output = tokenizer.decode(s, skip_special_tokens=True)\n",
    "    print(output)\n",
    "\n",
    "    return text\n",
    "# input_text = gr.components.Textbox(\n",
    "#             lines=2, label=\"Instruction\", placeholder=\"Tell me about alpacas.\"\n",
    "#         )\n",
    "# # tts_button = gr.Button(\"Change to voice\", elem_id=\"send-btn\", visible=True)\n",
    "\n",
    "# output_audio = gr.outputs.Audio(label=\"Output\", type=\"filepath\")\n",
    "\n",
    "# g = gr.Interface(\n",
    "#     fn=evaluate,\n",
    "#     inputs=[\n",
    "#         input_text,\n",
    "#         gr.components.Textbox(lines=2, label=\"Input\", placeholder=\"none\"),\n",
    "#         gr.components.Slider(minimum=0, maximum=1, value=0.1, label=\"Temperature\"),\n",
    "#         gr.components.Slider(minimum=0, maximum=1, value=0.75, label=\"Top p\"),\n",
    "#         gr.components.Slider(minimum=0, maximum=100, step=1, value=40, label=\"Top k\"),\n",
    "#         gr.components.Slider(minimum=1, maximum=4, step=1, value=4, label=\"Beams\"),\n",
    "#         gr.components.Slider(\n",
    "#             minimum=1, maximum=512, step=1, value=128, label=\"Max tokens\"\n",
    "#         ),\n",
    "#     ],\n",
    "#     outputs=[\n",
    "#         gr.inputs.Textbox(\n",
    "#             lines=5,\n",
    "#             label=\"Output\",\n",
    "#         ),\n",
    "#         output_audio\n",
    "#     ],\n",
    "#     title=\"üåäüê¢ OdiaGenAI-4-bit\",\n",
    "#     description=\"OdiaGenAI-4-bit is a 7B-parameter LLaMA model finetuned to follow Odia instructions. It is trained on the [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) Odia translated dataset and makes use of the Huggingface LLaMA implementation. For more information, please visit [the project's website](https://www.odiagenai.org/).\",\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# g.queue(concurrency_count=2)\n",
    "# g.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(prompt_template, question, tokenizer):\n",
    "    return prompt_template.format(question, context, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompt):\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=1024, use_cache=True)\n",
    "    responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    # print(responses)\n",
    "    response_start = responses.find(\"### Response:\") + len(\"### Response:\")\n",
    "    response = responses[response_start:].strip()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_template = \"\"\"Below is an instruction in Bengali language that describes a task, paired with an input also in Bengali language that provides further context. Write a response in Bengali language that appropriately completes the request.\n",
    "\n",
    "# ### Instruction:\n",
    "# {}\n",
    "\n",
    "# ### Input:\n",
    "# {}\n",
    "\n",
    "# ### Response:\n",
    "# {}\n",
    "# \"\"\"\n",
    "prompt_template = \"\"\"‡¶®‡ßÄ‡¶ö‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶ï‡¶æ‡¶ú‡ßá‡¶∞ ‡¶¨‡¶∞‡ßç‡¶£‡¶®‡¶æ ‡¶∏‡¶π ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶Ø‡¶º ‡¶è‡¶ï‡¶ü‡¶ø ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡ßá‡¶∂‡¶®‡¶æ ‡¶¶‡ßá‡¶ì‡¶Ø‡¶º‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá, ‡¶Ø‡¶æ ‡¶Ü‡¶∞‡¶ì ‡¶™‡ßç‡¶∞‡¶∏‡¶ô‡ßç‡¶ó ‡¶™‡ßç‡¶∞‡¶¶‡¶æ‡¶® ‡¶ï‡¶∞‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶á‡¶®‡¶™‡ßÅ‡¶ü ‡¶∏‡¶π ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶Ø‡¶º ‡¶∞‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶Ø‡¶º ‡¶è‡¶ï‡¶ü‡¶ø ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶≤‡¶ø‡¶ñ‡ßÅ‡¶® ‡¶Ø‡¶æ ‡¶Ö‡¶®‡ßÅ‡¶∞‡ßã‡¶ß‡¶ü‡¶ø ‡¶â‡¶™‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶≠‡¶æ‡¶¨‡ßá ‡¶∏‡¶Æ‡ßç‡¶™‡¶®‡ßç‡¶® ‡¶ï‡¶∞‡ßá‡•§\n",
    "\n",
    "### ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡ßá‡¶∂‡¶®‡¶æ:\n",
    "{}\n",
    "\n",
    "### ‡¶á‡¶®‡¶™‡ßÅ‡¶ü:\n",
    "{}\n",
    "\n",
    "### ‡¶â‡¶§‡ßç‡¶§‡¶∞:\n",
    "{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========PROMPT============\n",
      "‡¶®‡ßÄ‡¶ö‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶ï‡¶æ‡¶ú‡ßá‡¶∞ ‡¶¨‡¶∞‡ßç‡¶£‡¶®‡¶æ ‡¶∏‡¶π ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶Ø‡¶º ‡¶è‡¶ï‡¶ü‡¶ø ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡ßá‡¶∂‡¶®‡¶æ ‡¶¶‡ßá‡¶ì‡¶Ø‡¶º‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá, ‡¶Ø‡¶æ ‡¶Ü‡¶∞‡¶ì ‡¶™‡ßç‡¶∞‡¶∏‡¶ô‡ßç‡¶ó ‡¶™‡ßç‡¶∞‡¶¶‡¶æ‡¶® ‡¶ï‡¶∞‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶á‡¶®‡¶™‡ßÅ‡¶ü ‡¶∏‡¶π ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶Ø‡¶º ‡¶∞‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶Ø‡¶º ‡¶è‡¶ï‡¶ü‡¶ø ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶≤‡¶ø‡¶ñ‡ßÅ‡¶® ‡¶Ø‡¶æ ‡¶Ö‡¶®‡ßÅ‡¶∞‡ßã‡¶ß‡¶ü‡¶ø ‡¶â‡¶™‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶≠‡¶æ‡¶¨‡ßá ‡¶∏‡¶Æ‡ßç‡¶™‡¶®‡ßç‡¶® ‡¶ï‡¶∞‡ßá‡•§\n",
      "\n",
      "### ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡ßá‡¶∂‡¶®‡¶æ:\n",
      "‡¶≠‡¶æ‡¶∞‡¶§‡ßÄ‡¶Ø‡¶º ‡¶¨‡¶æ‡¶ô‡¶æ‡¶≤‡¶ø ‡¶ï‡¶•‡¶æ‡¶∏‡¶æ‡¶π‡¶ø‡¶§‡ßç‡¶Ø‡¶ø‡¶ï ‡¶Æ‡¶π‡¶æ‡¶∂‡ßç‡¶¨‡ßá‡¶§‡¶æ ‡¶¶‡ßá‡¶¨‡ßÄ‡¶∞ ‡¶Æ‡ßÉ‡¶§‡ßç‡¶Ø‡ßÅ ‡¶ï‡¶¨‡ßá ‡¶π‡ßü ?\n",
      "\n",
      "### ‡¶á‡¶®‡¶™‡ßÅ‡¶ü:\n",
      "‡ß®‡ß¶‡ßß‡ß¨ ‡¶∏‡¶æ‡¶≤‡ßá‡¶∞ ‡ß®‡ß© ‡¶ú‡ßÅ‡¶≤‡¶æ‡¶á ‡¶π‡ßÉ‡¶¶‡¶∞‡ßã‡¶ó‡ßá ‡¶Ü‡¶ï‡ßç‡¶∞‡¶æ‡¶®‡ßç‡¶§ ‡¶π‡¶Ø‡¶º‡ßá ‡¶Æ‡¶π‡¶æ‡¶∂‡ßç‡¶¨‡ßá‡¶§‡¶æ ‡¶¶‡ßá‡¶¨‡ßÄ ‡¶ï‡¶≤‡¶ï‡¶æ‡¶§‡¶æ‡¶∞ ‡¶¨‡ßá‡¶≤ ‡¶≠‡¶ø‡¶â ‡¶ï‡ßç‡¶≤‡¶ø‡¶®‡¶ø‡¶ï‡ßá ‡¶≠‡¶∞‡ßç‡¶§‡¶ø ‡¶π‡¶®‡•§ ‡¶∏‡ßá‡¶á ‡¶¨‡¶õ‡¶∞‡¶á ‡ß®‡ßÆ ‡¶ú‡ßÅ‡¶≤‡¶æ‡¶á ‡¶è‡¶ï‡¶æ‡¶ß‡¶ø‡¶ï ‡¶Ö‡¶ô‡ßç‡¶ó ‡¶¨‡¶ø‡¶ï‡¶≤ ‡¶π‡¶Ø‡¶º‡ßá ‡¶§‡¶æ‡¶Å‡¶∞ ‡¶Æ‡ßÉ‡¶§‡ßç‡¶Ø‡ßÅ ‡¶ò‡¶ü‡ßá‡•§ ‡¶§‡¶ø‡¶®‡¶ø ‡¶Æ‡¶ß‡ßÅ‡¶Æ‡ßá‡¶π, ‡¶∏‡ßá‡¶™‡ßç‡¶ü‡¶ø‡¶∏‡ßá‡¶Æ‡¶ø‡¶Ø‡¶º‡¶æ ‡¶ì ‡¶Æ‡ßÇ‡¶§‡ßç‡¶∞ ‡¶∏‡¶Ç‡¶ï‡ßç‡¶∞‡¶Æ‡¶£ ‡¶∞‡ßã‡¶ó‡ßá‡¶ì ‡¶≠‡ßÅ‡¶ó‡¶õ‡¶ø‡¶≤‡ßá‡¶®‡•§\n",
      "\n",
      "### ‡¶â‡¶§‡ßç‡¶§‡¶∞:\n",
      "\n",
      "\n",
      "==========RESPONSE============\n",
      " ‡¶®‡ßÄ‡¶ö‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶ï‡¶æ‡¶ú‡ßá‡¶∞ ‡¶¨‡¶∞‡ßç‡¶£‡¶®‡¶æ ‡¶∏‡¶π ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶Ø‡¶º ‡¶è‡¶ï‡¶ü‡¶ø ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡ßá‡¶∂‡¶®‡¶æ ‡¶¶‡ßá‡¶ì‡¶Ø‡¶º‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá, ‡¶Ø‡¶æ ‡¶Ü‡¶∞‡¶ì ‡¶™‡ßç‡¶∞‡¶∏‡¶ô‡ßç‡¶ó ‡¶™‡ßç‡¶∞‡¶¶‡¶æ‡¶® ‡¶ï‡¶∞‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶á‡¶®‡¶™‡ßÅ‡¶ü ‡¶∏‡¶π ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶Ø‡¶º ‡¶∞‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶Ø‡¶º ‡¶è‡¶ï‡¶ü‡¶ø ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶≤‡¶ø‡¶ñ‡ßÅ‡¶® ‡¶Ø‡¶æ ‡¶Ö‡¶®‡ßÅ‡¶∞‡ßã‡¶ß‡¶ü‡¶ø ‡¶â‡¶™‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶≠‡¶æ‡¶¨‡ßá ‡¶∏‡¶Æ‡ßç‡¶™‡¶®‡ßç‡¶® ‡¶ï‡¶∞‡ßá‡•§\n",
      "\n",
      "### ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡ßá‡¶∂‡¶®‡¶æ:\n",
      "‡¶≠‡¶æ‡¶∞‡¶§‡ßÄ‡¶Ø‡¶º ‡¶¨‡¶æ‡¶ô‡¶æ‡¶≤‡¶ø ‡¶ï‡¶•‡¶æ‡¶∏‡¶æ‡¶π‡¶ø‡¶§‡ßç‡¶Ø‡¶ø‡¶ï ‡¶Æ‡¶π‡¶æ‡¶∂‡ßç‡¶¨‡ßá‡¶§‡¶æ ‡¶¶‡ßá‡¶¨‡ßÄ‡¶∞ ‡¶Æ‡ßÉ‡¶§‡ßç‡¶Ø‡ßÅ ‡¶ï‡¶¨‡ßá ‡¶π‡ßü ?\n",
      "\n",
      "### ‡¶á‡¶®‡¶™‡ßÅ‡¶ü:\n",
      "‡ß®‡ß¶‡ßß‡ß¨ ‡¶∏‡¶æ‡¶≤‡ßá‡¶∞ ‡ß®‡ß© ‡¶ú‡ßÅ‡¶≤‡¶æ‡¶á ‡¶π‡ßÉ‡¶¶‡¶∞‡ßã‡¶ó‡ßá ‡¶Ü‡¶ï‡ßç‡¶∞‡¶æ‡¶®‡ßç‡¶§ ‡¶π‡¶Ø‡¶º‡ßá ‡¶Æ‡¶π‡¶æ‡¶∂‡ßç‡¶¨‡ßá‡¶§‡¶æ ‡¶¶‡ßá‡¶¨‡ßÄ ‡¶ï‡¶≤‡¶ï‡¶æ‡¶§‡¶æ‡¶∞ ‡¶¨‡ßá‡¶≤ ‡¶≠‡¶ø‡¶â ‡¶ï‡ßç‡¶≤‡¶ø‡¶®‡¶ø‡¶ï‡ßá ‡¶≠‡¶∞‡ßç‡¶§‡¶ø ‡¶π‡¶®‡•§ ‡¶∏‡ßá‡¶á ‡¶¨‡¶õ‡¶∞‡¶á ‡ß®‡ßÆ ‡¶ú‡ßÅ‡¶≤‡¶æ‡¶á ‡¶è‡¶ï‡¶æ‡¶ß‡¶ø‡¶ï ‡¶Ö‡¶ô‡ßç‡¶ó ‡¶¨‡¶ø‡¶ï‡¶≤ ‡¶π‡¶Ø‡¶º‡ßá ‡¶§‡¶æ‡¶Å‡¶∞ ‡¶Æ‡ßÉ‡¶§‡ßç‡¶Ø‡ßÅ ‡¶ò‡¶ü‡ßá‡•§ ‡¶§‡¶ø‡¶®‡¶ø ‡¶Æ‡¶ß‡ßÅ‡¶Æ‡ßá‡¶π, ‡¶∏‡ßá‡¶™‡ßç‡¶ü‡¶ø‡¶∏‡ßá‡¶Æ‡¶ø‡¶Ø‡¶º‡¶æ ‡¶ì ‡¶Æ‡ßÇ‡¶§‡ßç‡¶∞ ‡¶∏‡¶Ç‡¶ï‡ßç‡¶∞‡¶Æ‡¶£ ‡¶∞‡ßã‡¶ó‡ßá‡¶ì ‡¶≠‡ßÅ‡¶ó‡¶õ‡¶ø‡¶≤‡ßá‡¶®‡•§\n",
      "\n",
      "### ‡¶â‡¶§‡ßç‡¶§‡¶∞:\n",
      "\n",
      "‡ß®‡ß¶‡ßß‡ß¨ ‡¶∏‡¶æ‡¶≤‡ßá‡¶∞ ‡ß®‡ß© ‡¶ú‡ßÅ‡¶≤‡¶æ‡¶á ‡¶π‡ßÉ‡¶¶‡¶∞‡ßã‡¶ó‡ßá ‡¶Ü‡¶ï‡ßç‡¶∞‡¶æ‡¶®‡ßç‡¶§ ‡¶π‡¶Ø‡¶º‡ßá ‡¶Æ‡¶π‡¶æ‡¶∂‡ßç‡¶¨‡ßá‡¶§‡¶æ ‡¶¶‡ßá‡¶¨‡ßÄ ‡¶ï‡¶≤‡¶ï‡¶æ‡¶§‡¶æ‡¶∞ ‡¶¨‡ßá‡¶≤ ‡¶≠‡¶ø‡¶â ‡¶ï‡ßç‡¶≤‡¶ø‡¶®‡¶ø‡¶ï‡ßá ‡¶≠‡¶∞‡ßç‡¶§‡¶ø ‡¶π‡¶Ø‡¶º‡•§ ‡¶∏‡ßá‡¶á ‡¶¨‡¶õ‡¶∞\n"
     ]
    }
   ],
   "source": [
    "temperature=0.1\n",
    "top_p=0.75\n",
    "top_k=40\n",
    "num_beams=4\n",
    "max_new_tokens=128\n",
    "device = 'cuda'\n",
    "prompt = generate_prompt(prompt_template, question, context)\n",
    "# prompt = '‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶Ø‡¶¶‡¶ø ‡¶ö‡¶æ‡¶ì ‡¶§‡¶¨‡ßá'\n",
    "print('==========PROMPT============')\n",
    "print(prompt)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "# print(inputs)\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "# print(input_ids)\n",
    "generation_config = transformers.GenerationConfig(\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    top_k=top_k,\n",
    "    num_beams=num_beams,\n",
    "    max_new_tokens = max_new_tokens,\n",
    "    do_sample = True\n",
    "    # **kwargs,\n",
    ")\n",
    "with torch.no_grad():\n",
    "    generation_output = lora_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "    )\n",
    "    # print(generation_output)\n",
    "s = generation_output.sequences[0]\n",
    "# print(s)\n",
    "output = tokenizer.decode(s, skip_special_tokens=True)\n",
    "print('==========RESPONSE============')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:3')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_output.scores[10]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1e236e1dad0c411dbe4aa5e8c31694a1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "222ec815e1b04fafb94d7cf81a047b12": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39ff2366b890492ab3c62d405a2b3274": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d67a75f9924647959277a99156825b93",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_936c86ed69c3484681d9bd5a736d134b",
      "value": " 39/39 [01:17&lt;00:00,  1.77s/it]"
     }
    },
    "4236dcba501f4eb38ecc4edaa237da21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_48274de1ad484519ab406a162f1daf58",
       "IPY_MODEL_67175ae3885a453fbccb9c455638547f",
       "IPY_MODEL_39ff2366b890492ab3c62d405a2b3274"
      ],
      "layout": "IPY_MODEL_6081eb420cb447e2baacc0e7804b7f53"
     }
    },
    "48274de1ad484519ab406a162f1daf58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e236e1dad0c411dbe4aa5e8c31694a1",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_ec140945db204424afe0d801babffe60",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "5e0a638e8bc340c98e52642bf8c90fa2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6081eb420cb447e2baacc0e7804b7f53": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "67175ae3885a453fbccb9c455638547f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_222ec815e1b04fafb94d7cf81a047b12",
      "max": 39,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5e0a638e8bc340c98e52642bf8c90fa2",
      "value": 39
     }
    },
    "936c86ed69c3484681d9bd5a736d134b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d67a75f9924647959277a99156825b93": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ec140945db204424afe0d801babffe60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
